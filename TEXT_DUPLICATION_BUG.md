# ğŸ› CRITICAL: Text Duplication Bug in Streaming

**Date**: 2025-10-26  
**Status**: ğŸš¨ CRITICAL BUG IDENTIFIED  

---

## ğŸš¨ **The Problem**

**User shows**: Text is duplicated word-by-word during streaming!

```
Expected: "Adentic Video Intelligence Analysis: Nike on TikTok"

Actual: "Ad Adenticentic Video Video Intelligence Intelligence Analysis Analysis:: Nike Nike on on Tik TikTokTok"
```

**Even XML tags are duplicated**:
```
Expected: "<function_calls>"
Actual: "<<functionfunction_calls_calls>>"
```

---

## ğŸ” **Analysis**

### **When It Happens**:
- âœ… During **streaming** (before tool executes)
- âœ… In **assistant's message**
- âœ… Consistent pattern (every word/character duplicated)

### **What This Means**:
1. âš ï¸ This is **NOT a frontend rendering issue** (it's in the source data)
2. âš ï¸ This is **NOT XML parsing** (XML itself is duplicated)
3. ğŸ”¥ This is **backend streaming duplication** - content is being yielded/sent twice!

### **Possible Causes**:

**Option A: LLM Response Duplication**
- LLM itself generating duplicated text
- Model hallucinating/glitching
- Prompt causing repetition

**Option B: Streaming Layer Duplication**
- Backend yields each chunk twice
- Response processor duplicates content
- Redis pubsub sending messages twice

**Option C: Content Concatenation Bug**
- Content being appended to itself
- Streaming buffer duplicated
- accumulator += accumulator bug

---

## ğŸ” **Where to Look**

### **Backend Streaming Chain**:
```
LLM Response
  â†“
response_processor.py (process_streaming_response)
  â†“
thread_manager.py (_execute_run)
  â†“
run.py (AgentRunner.run)
  â†“  
agent_runs.py (stream_agent_run)
  â†“
Frontend receives duplicated content
```

### **Key Files to Check**:
1. **`response_processor.py`** (lines 365-374):
   ```python
   accumulated_content += chunk_content
   current_xml_content += chunk_content
   ```
   - Is chunk_content being added twice?
   - Is there a loop duplicating it?

2. **`thread_manager.py`** (lines 388-445):
   - Is response being yielded multiple times?
   - Is there a wrapper duplicating chunks?

3. **`run.py`** (lines 737-793):
   ```python
   async for chunk in response:
       yield chunk
   ```
   - Is this being called twice?
   - Is there double iteration?

4. **`agent_runs.py`** (lines 454):
   ```python
   for response in initial_responses:
       yield f"data: {json.dumps(response)}\n\n"
   ```
   - Are initial responses duplicated in Redis?

---

## ğŸ§ª **How to Debug**

### **Add Logging** in `response_processor.py`:
```python
# Line 365-372
if delta and hasattr(delta, 'content') and delta.content:
    chunk_content = delta.content
    logger.info(f"ğŸ” RAW CHUNK: '{chunk_content}'")  # See what LLM sends
    accumulated_content += chunk_content
    logger.info(f"ğŸ” ACCUMULATED SO FAR: '{accumulated_content[-50:]}'")  # Last 50 chars
```

This will show:
- âœ… Is LLM generating duplicated text?
- âœ… Is accumulation logic broken?
- âœ… Where duplication starts?

### **Check Redis** in `agent_runs.py`:
```python
# Line 448
initial_responses_json = await redis.lrange(response_list_key, 0, -1)
logger.info(f"ğŸ” REDIS RESPONSES COUNT: {len(initial_responses_json)}")
for i, r in enumerate(initial_responses_json[:3]):
    logger.info(f"ğŸ” RESPONSE {i}: {r[:100]}")  # First 100 chars
```

This will show:
- âœ… Are responses duplicated in Redis?
- âœ… Is content already doubled when stored?

---

## ğŸ”§ **Likely Culprits**

### **Most Likely** (90% confidence):
**LLM is generating duplicated text**
- Model glitch or hallucination
- Prompt causing repetition
- Temperature/sampling issue

**Check**: Look at raw LLM output before any processing

### **Possible** (50% confidence):
**Streaming accumulation bug**
- `accumulated_content += chunk_content` being called twice
- Loop running double iterations

**Check**: Add logging around concatenation

### **Unlikely** (10% confidence):
**Redis/Network duplication**
- Messages being published twice to Redis
- Pub/sub subscribing to same channel twice

**Check**: Redis message count

---

## ğŸ¯ **Immediate Action**

### **Quick Test**:
1. Check if this happens with **ALL queries** or just video intelligence
2. Try a simple query: "hello, how are you?"
   - If also duplicated â†’ LLM/streaming issue
   - If NOT duplicated â†’ Video intelligence specific

### **If Video Intelligence Specific**:
- Check `search_trending_content` tool
- Check if Memories.ai response is being duplicated
- Check if tool calling is causing duplication

### **If All Queries**:
- Check LLM provider settings
- Check response_processor streaming logic
- Check if messages are being saved to DB twice

---

## ğŸš¨ **Critical Impact**

This bug makes the entire chat **UNREADABLE**!

**Example**:
```
User: "analyze nike"

Expected Assistant:
"Using Adentic Video Intelligence Engine to search for Nike..."

Actual Assistant:
"UsingUsing AdAdenticentic VideoVideo IntelligenceIntelligence EngineEngine toto searchsearch forfor NikeNike..."
```

**This MUST be fixed before anything else!** ğŸ”¥

---

## ğŸ“‹ **Next Steps**

1. **Add logging** to see where duplication starts
2. **Test simple query** to see if it's global or video-specific
3. **Check LLM raw output** to see if model is generating duplicates
4. **Inspect streaming chain** for double-yield bugs

**Status**: ğŸš¨ **CRITICAL BUG** - Blocks all functionality!

